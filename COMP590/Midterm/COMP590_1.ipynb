{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP590-1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQeH6ulb30-I"
      },
      "source": [
        "# Caption with Attention: Encoder-Decoder Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-B_H-_4JI7G"
      },
      "source": [
        "# Contents\n",
        "\n",
        "#     CLASSES:\n",
        "#         FlickrDataset\n",
        "#         Vocabulary\n",
        "#         Encoder\n",
        "#         Attention\n",
        "#         Decoder\n",
        "#         CaptionModel\n",
        "\n",
        "#     STRUCTURE:\n",
        "#         \\Preprocessing/\n",
        "#             FlickrDataset\n",
        "#             Vocabulary\n",
        "#                 Pretrained Embedding\n",
        "#             get_loader(transform, loader(collate))\n",
        "#         \\Model/\n",
        "#             CaptionModel\n",
        "#                     |\n",
        "#                     |----Encoder: Resnet[:-2] and avgpool\n",
        "#                     |                 features and input cell states\n",
        "#                     |\n",
        "#                     |----Decoder: LSTM\n",
        "#                     |             GloVe Embedding\n",
        "#                     |             Attention\n",
        "#                     |\n",
        "#                     |----generate_caption()\n",
        "#         \\Training/Analysis/\n",
        "#             plot_attention(): Image stacked with attention scores on each block\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tPQhqJSf9Tj"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuqRJvhQgkQa"
      },
      "source": [
        "#!kill process_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKKVQJUva4av"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import math\n",
        "import sys\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from fractions import Fraction\n",
        "from nltk.util import ngrams\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AjG8-47zh8H"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxTgtYmEv-aY"
      },
      "source": [
        "# terminal command, change current directory\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/ImageCaption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgicHQWDt8bG"
      },
      "source": [
        "# Colab Only\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0LmJp1yoR2J"
      },
      "source": [
        "# Python Notebook Only\n",
        "\n",
        "# !pip3 install -U spacy\n",
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mskJzagM4ClB"
      },
      "source": [
        "## 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMcD94n4SIO"
      },
      "source": [
        "### 1.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di1RGeS9FdaB"
      },
      "source": [
        "# get_loader.py\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform, topk=10000):\n",
        "        self.root_dir = root_dir\n",
        "        self.separator = ',' if 'flickr8k' in root_dir else '|'\n",
        "        self.df = pd.read_csv(captions_file, sep=self.separator)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.img_ids = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        self.vocab = Vocabulary(topk)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.img_ids[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        \n",
        "        caption = self.captions[index]\n",
        "        numericalized = self.vocab.numericalize(caption)    # numericalize=tokenize+'numericalize'\n",
        "        numericalized = [self.vocab.stoi['<SOS>']] + numericalized\n",
        "        numericalized.append(self.vocab.stoi['<EOS>'])\n",
        "\n",
        "        return img, torch.tensor(numericalized)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9LaUGF24NJx"
      },
      "source": [
        "### 1.2 Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epu3D92hqgi9"
      },
      "source": [
        "# get_loader.py\n",
        "class Vocabulary:\n",
        "    def __init__(self, topk=20000):\n",
        "        self.topk = topk\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def tokenizer_eng(self, text):\n",
        "        return [tok.text.lower() for tok in self.nlp.tokenizer(text)]\n",
        "        \n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        all_tokens = []\n",
        "        count=0\n",
        "        print('Number of example loaded:')\n",
        "        for sentence in sentence_list:\n",
        "            if count % 10000 == 9999:\n",
        "                print(count+1)\n",
        "            count+=1\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                all_tokens.append(word)\n",
        "        print(len(sentence_list)+1)\n",
        "        counter = Counter(all_tokens)\n",
        "        most_common = counter.most_common(self.topk)   # list of of (token, count)\n",
        "        most_common = [tok for tok, count in most_common]\n",
        "        tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
        "        for num, tok in enumerate(tokens):\n",
        "            self.itos[num+4] = tok\n",
        "            self.stoi[tok] = num+4\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized = self.tokenizer_eng(text)\n",
        "        numericalized = [self.stoi[tok] if tok in self.stoi else self.stoi['<UNK>'] for tok in tokenized]\n",
        "        return numericalized\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq_dAAxL082_"
      },
      "source": [
        "# Load glove from local file and form a dict\n",
        "def get_glove_embedding():\n",
        "    glove = pd.read_csv('glove.6B.200d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
        "    glove_embedding = {key: val.values for key, val in glove.T.items()}\n",
        "    return glove_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnu3UJPw1-l4"
      },
      "source": [
        "# build embedding matrix according to dataset\n",
        "# embedding_dict\n",
        "\n",
        "def build_embedding_matrix(itos, embedding_dict):\n",
        "    embed_size = len(embedding_dict['a'])\n",
        "    embedding_matrix = torch.empty((len(itos),embed_size))\n",
        "    for index, word in enumerate(itos):\n",
        "        if word in embedding_dict:\n",
        "            embedding_matrix[index] = embedding_dict[word]\n",
        "        else:\n",
        "            torch.nn.init.normal_(embedding_matrix[index], std=0.4)\n",
        "    return embedding_matrix\n",
        "\n",
        "def create_embedding(embedding_matrix, trainable=False):\n",
        "    vocab_size = embedding_matrix.shape[0]\n",
        "    embed_size = embedding_matrix.shape[1]\n",
        "    embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
        "    embedding_layer.load_state_dict({'weight': embedding_matrix})\n",
        "    if not trainable:\n",
        "        embedding_layer.weight.requires_grad = False\n",
        "    return embedding_layer, embed_size, vocab_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCjhTtP04c6b"
      },
      "source": [
        "### 1.3 Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNPlDLPoR2N"
      },
      "source": [
        "# # Class: get_loader\n",
        "# def get_loader(root_folder, annotation_file, transform, ratio=0.8, batch_size=32, num_workers=2, shuffle=True, pin_memory=True):\n",
        "#     dataset = FlickrDataset(root_folder, annotation_file, transform=transform, topk=10000)\n",
        "#     #train_set, dev_set = torch.utils.data.random_split(dataset, [int(ratio*len(dataset)),len(dataset)-int(ratio*len(dataset))])\n",
        "#     pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "#     loader = DataLoader(\n",
        "#                         dataset=dataset,\n",
        "#                         batch_size=batch_size,\n",
        "#                         num_workers=num_workers,\n",
        "#                         shuffle=shuffle,\n",
        "#                         pin_memory=pin_memory,\n",
        "#                         # drop_last=True,\n",
        "#                         collate_fn=Collate(pad_idx=pad_idx)  # call Collate\n",
        "#     )\n",
        "#     return loader, dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW3nGD0zCKvi"
      },
      "source": [
        "# Class: get_loader\n",
        "def get_loader(root_folder, annotation_file, transform, ratio=0.8, batch_size=32, num_workers=1, pin_memory=True):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform, topk=10000)\n",
        "    #train_set, dev_set = torch.utils.data.random_split(dataset, [int(ratio*len(dataset)),len(dataset)-int(ratio*len(dataset))])\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(ratio * dataset_size))\n",
        "    split = (split//5)*5\n",
        "    # each image has 5 descriptions, make sure all images in train_set are not in dev_set\n",
        "    train_indices, dev_indices = indices[:split], indices[split:]\n",
        "\n",
        "    # Creating PT data samplers and loaders:\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    dev_sampler = SubsetRandomSampler(dev_indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "                        dataset=dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=num_workers,\n",
        "                        pin_memory=pin_memory,\n",
        "                        sampler=train_indices,\n",
        "                        collate_fn=Collate(pad_idx=pad_idx)  # call Collate\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "                        dataset=dataset,\n",
        "                        batch_size=5,\n",
        "                        num_workers=num_workers,\n",
        "                        pin_memory=pin_memory,\n",
        "                        sampler=dev_indices,\n",
        "                        collate_fn=Collate(pad_idx=pad_idx)  # call Collate\n",
        "    )\n",
        "\n",
        "    return train_loader, dev_loader, dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxzTDU3g41MR"
      },
      "source": [
        "### 1.4 Utils for Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXGadXUs4k5Z"
      },
      "source": [
        "# Class: get_loader\n",
        "class Collate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        # print('img_pre:',type(imgs))\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        # print('img_post:',type(imgs))\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "        # print('targets.shape:', targets.shape)\n",
        "        return imgs, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22YHYmbUP5Xo"
      },
      "source": [
        "# Class: get_loader\n",
        "def get_transform():\n",
        "    transform = transforms.Compose([\n",
        "                                   transforms.Resize(224), # transforms.Resize(256)\n",
        "                                   transforms.CenterCrop(224), # transforms.RandomCrop(224)\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
        "                                  ])\n",
        "    return transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcbdQYiv4qAi"
      },
      "source": [
        "## 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El9Y93fPL1X3"
      },
      "source": [
        "### 2.1 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAuYQ4rgeoxX"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, freeze_resnet=True, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.ResNet_feature = nn.Sequential(*(list(torchvision.models.resnet50(pretrained=True).children())[:-2]))\n",
        "        self.avgpool = torchvision.models.resnet50(pretrained=True).avgpool\n",
        "        self.linear = nn.Linear(2048, hidden_size) # pass in to cell_state\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        if freeze_resnet:\n",
        "            for param in self.ResNet_feature.parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def forward(self, images):\n",
        "        features = self.ResNet_feature(images)\n",
        "        before_fc = self.avgpool(features)\n",
        "        input_cell = F.relu(self.linear(before_fc.reshape(-1,2048)))  # batch x 2048\n",
        "        input_cell = self.dropout(input_cell) # batch x hidden_size\n",
        "\n",
        "        # features shape: (b,2048,7,7)=>(b,2048,49)=>(b,49,2048)\n",
        "        features = features.reshape(features.shape[0], 2048, 49).permute(0,2,1)\n",
        "        return features, input_cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCG664IOL5QH"
      },
      "source": [
        "### 2.2 Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "risKqkKOJT_g"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W_f = nn.Linear(2048, units)\n",
        "        self.W_s = nn.Linear(2*hidden_size, units)\n",
        "        #self.W_s = nn.Linear(2*hidden_size, 49)\n",
        "        self.W_att = nn.Linear(units, 1)\n",
        "\n",
        "    def forward(self, features, states):\n",
        "        # features = (b, 49, 2048)\n",
        "        # states = ((1, b, h), (1, b, h))\n",
        "        states = torch.cat(states, dim=-1) # (1, b, h*2)\n",
        "        feature_units = self.W_f(features) #(b, 49, units)\n",
        "\n",
        "        ##### with attention #####\n",
        "        attention = torch.tanh(feature_units+self.W_s(states.permute(1,0,2))) # (b, 49, units) broadcast\n",
        "        ##########################\n",
        "\n",
        "        ##### without interaction, only feature input #####\n",
        "        # attention = torch.tanh(feature_units) # (b, 49, units) broadcast\n",
        "        ####################################################\n",
        "\n",
        "        \n",
        "        # (b, 1, units) broadcast\n",
        "        scores = self.W_att(attention)  # b x 49 x 1\n",
        "        attention_weights = F.softmax(scores, dim=1)  # b x 49 x 1\n",
        "        context_vector = attention_weights * feature_units # b x 49 x units, broadcast\n",
        "        context_vector = torch.sum(context_vector, dim=1) # b x units\n",
        "        return context_vector, attention_weights  # (b, units), (b x 49 x 1)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az4yxn7KL8xZ"
      },
      "source": [
        "### 2.3 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRz6ZVtIqtDD"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, units, vocab_size, freeze_embedding=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.units = units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.lstm = nn.LSTM(\n",
        "                            input_size=embed_size + units,\n",
        "                            hidden_size=hidden_size,\n",
        "                            batch_first=True\n",
        "                            )\n",
        "        self.linear = nn.Linear(hidden_size, self.vocab_size)\n",
        "        self.attention = Attention(hidden_size, self.units)\n",
        "\n",
        "    def forward(self, word_vector, states, features):\n",
        "        # word_vector (b, 1, emb)\n",
        "        # states ((1, b, h), (1, b, h))\n",
        "        context_vector, attention_weights = self.attention(features, states) # (b, units), (b x 49 x 1)\n",
        "        context_vector = context_vector.unsqueeze(1)  # (b, 1, units)\n",
        "        input = torch.cat((word_vector, context_vector), dim=-1) # (b, 1, emb+units)\n",
        "        output, states = self.lstm(input, states)  # output:(b, 1, h), ((1, b, h), (1, b, h))\n",
        "        logits = self.linear(output) # (b, 1, vocab)\n",
        "\n",
        "        ####### ******** ########\n",
        "        return states, logits, attention_weights\n",
        "        ####### ******** ########\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OruVWhEHMBtY"
      },
      "source": [
        "### 2.A Complete Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20k3Ki6r_2Pr"
      },
      "source": [
        "class CaptionModel(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, units,\n",
        "                 freeze_resnet=True, freeze_embedding=True, dropout=0.2):\n",
        "        super(CaptionModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.units = units\n",
        "        self.embedding, self.embed_size, self.vocab_size = create_embedding(embedding_matrix, trainable=not freeze_embedding)\n",
        "        \n",
        "        #self.embedding.weight.data.normal_()\n",
        "        \n",
        "        self.encoder = Encoder(self.units, freeze_resnet=freeze_resnet, dropout=dropout)\n",
        "        self.decoder = Decoder(self.embed_size, hidden_size, units, vocab_size=self.vocab_size,\n",
        "                               freeze_embedding=freeze_embedding)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    def forward(self, images, captions):\n",
        "        features, input_cell = self.encoder(images) #(b, 49, 2048), (b, hidden)\n",
        "        input_cell = input_cell.unsqueeze(0) # (1, b, hidden)\n",
        "        #input_cell = torch.zeros_like(input_cell.unsqueeze(0)) # (1, b, hidden)\n",
        "        input_hidden = torch.zeros_like(input_cell)\n",
        "        states = (input_hidden, input_cell)\n",
        "\n",
        "        # all_logits:(b, length, vocab_size)\n",
        "        all_logits = torch.zeros((captions.shape[0], captions.shape[1], self.vocab_size)).to(device)\n",
        "        embeddings = self.embedding(captions) #embeddings:(b, length, emb)\n",
        "        for idx in range(captions.shape[1]): #captions:(b, length)\n",
        "            word_vector = embeddings[:, idx, :].unsqueeze(1)  # (b, 1, emb)\n",
        "            states, logits, _ = self.decoder(word_vector, states, features)  # logits:(b, 1, vocab)\n",
        "            all_logits[:, idx, :] = logits.squeeze(1)\n",
        "\n",
        "        return all_logits  # (b, length, vocab_size)\n",
        "\n",
        "    def generate_caption(self, images, vocab, max_length=30):\n",
        "        features, input_cell = self.encoder(images) #(b, 49, 2048), (b, hidden)\n",
        "        input_cell = input_cell.unsqueeze(0) # (1, b, hidden)\n",
        "        #input_cell = torch.zeros_like(input_cell.unsqueeze(0))\n",
        "        input_hidden = torch.zeros_like(input_cell)\n",
        "        states = (input_hidden, input_cell)\n",
        "\n",
        "        # predicted_caption = (b, l)\n",
        "        predicted_caption = torch.zeros((images.shape[0], max_length)) # fill with 0 => '<PAD>'\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        single_word_vector = self.embedding(torch.tensor(vocab.stoi['<SOS>']).to(device)) # (1, emb)\n",
        "        word_vector = single_word_vector.unsqueeze(0).repeat(batch_size, 1, 1) #(b, 1, emb)\n",
        "        attentions = torch.zeros((images.shape[0], max_length, 49)) #(b, max_1ength, 49)\n",
        "\n",
        "        for idx in range(max_length): #captions:(b, length)\n",
        "            states, logits, attention_weights = self.decoder(word_vector, states, features)  # logits:(b, 1, vocab)\n",
        "            # (b, 49, 1) => (b, 49)\n",
        "            attention_weights = attention_weights.squeeze(-1)\n",
        "            attentions[:, idx, :] = attention_weights\n",
        "            predicted = torch.argmax(logits, dim=-1) # predicted:(b, 1)\n",
        "            predicted_caption[:, idx] = predicted.squeeze(1) \n",
        "            word_vector = self.embedding(predicted) # pass in (b, 1), get (b, 1, emb)\n",
        "\n",
        "        # caption_list: (b, max_length)\n",
        "        caption_list = predicted_caption.tolist()\n",
        "        attention_array = attentions.detach().numpy() #(b, max_1ength, 49)\n",
        "\n",
        "        result_list = []\n",
        "        attention_list = []\n",
        "        for batch_idx in range(len(caption_list)):\n",
        "            line = caption_list[batch_idx]\n",
        "            line_result = []\n",
        "            for idx in range(len(line)):\n",
        "                num = line[idx]\n",
        "                if vocab.itos[num] != '<EOS>':\n",
        "                    line_result.append(vocab.itos[num])\n",
        "                else:\n",
        "                    append_att = attention_array[batch_idx, :idx, :].reshape(idx, 7, 7) \n",
        "                    attention_list.append(append_att) #[b]*(*real_length, 7, 7)\n",
        "                    break\n",
        "            result_list.append(line_result) # [b, real_length]\n",
        "        return result_list, attention_list\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_afYYur5Mte"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAKBjEVWMR6c"
      },
      "source": [
        "### 3.1 Train/Evaluate Fuction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuLNfDnc5QGH"
      },
      "source": [
        "# Define Train Function\n",
        "\n",
        "def train(model, device, loader, optimizer, criterion, epoch, log_interval=1, dry_run=False, save=False, ratio=0.8):\n",
        "    model.to(device)\n",
        "    criterion.to(device)\n",
        "    model.train()\n",
        "    best_loss = np.inf\n",
        "    for ep in range(1, epoch+1):\n",
        "        current = 0\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data, target[:,:-1])\n",
        "            loss = criterion(output.reshape(-1, output.shape[2]), target[:,1:].reshape(-1)) #(b*l, vocab), (b*l)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            current += len(data)\n",
        "            if batch_idx % log_interval == 0:\n",
        "                total = int(np.floor(ratio * len(loader.dataset))-5)\n",
        "                print('Train Epoch: {} \\t[{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    ep, current, total,\n",
        "                    100. * current / total, loss.item()))\n",
        "                if dry_run:\n",
        "                    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKI8S0FRHggz"
      },
      "source": [
        "# Define Evaluate Function\n",
        "\n",
        "def evaluate(model, device, loader, vocab, log_interval=5, ratio=0.8):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # batch_size = 5\n",
        "    sum_scores = [0,0,0,0]\n",
        "    count = 0\n",
        "    current = 0\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ### only input the image once ###\n",
        "        result_list, _ = model.generate_caption(data[0].unsqueeze(0), vocab)\n",
        "\n",
        "        target = target.tolist()\n",
        "        #print('target', target)\n",
        "\n",
        "        target_list = []\n",
        "        for line_idx in range(len(target)):\n",
        "            line = target[line_idx]\n",
        "            line_result = []\n",
        "            for word_idx in range(1,len(line)): # skip '<SOS>'\n",
        "                num = line[word_idx]\n",
        "                if vocab.itos[num] != '<EOS>':\n",
        "                    line_result.append(vocab.itos[num])\n",
        "                else:\n",
        "                    break\n",
        "            target_list.append(line_result)\n",
        "        target_list = [target_list]\n",
        "\n",
        "        # result_list: (1, length)\n",
        "        candidate_corpus = result_list # [[sentence]]\n",
        "        # targets: (b, length)\n",
        "        references_corpus = target_list #[[[sentence1], [sentence2], etc]]\n",
        "        # print('candidate_corpus', result_list)\n",
        "        # print('references_corpus', target_list)\n",
        "\n",
        "        for i in range(1,5):\n",
        "            score = bleu_score(candidate_corpus, references_corpus, max_n=i, weights = i * [1/i])\n",
        "            sum_scores[i-1] += score\n",
        "\n",
        "        count += 1\n",
        "        current += len(data)\n",
        "        if batch_idx % log_interval == 0:\n",
        "            total = len(loader.dataset)-np.floor(ratio * len(loader.dataset))\n",
        "            print('[{}/{} ({:.0f}%)]'.format(\n",
        "                current, total,\n",
        "                100. * current / total))\n",
        "    for i in range(1,5):\n",
        "        print(\"Bleu-\"+str(i)+\" score:\", round(sum_scores[i-1]/count, 3))\n",
        "    print(\"total count:\", count)\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Blp33LVAf1y"
      },
      "source": [
        "# 222\n",
        "# Define Evaluate Function 2\n",
        "\n",
        "def evaluate2(model, device, loader, vocab, log_interval=5, ratio=0.8):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # batch_size = 5\n",
        "    count = 0\n",
        "    current = 0\n",
        "    reference_list = []\n",
        "    hypothesis_list = []\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ### only input the image once ###\n",
        "        hypothesis, _ = model.generate_caption(data[0].unsqueeze(0), vocab)\n",
        "\n",
        "        target = target.tolist()\n",
        "        #print('target', target)\n",
        "\n",
        "        target_list = []\n",
        "        for line_idx in range(len(target)):\n",
        "            line = target[line_idx]\n",
        "            line_sent = []\n",
        "            for word_idx in range(1,len(line)): # skip '<SOS>'\n",
        "                num = line[word_idx]\n",
        "                if vocab.itos[num] != '<EOS>':\n",
        "                    line_sent.append(vocab.itos[num])\n",
        "                else:\n",
        "                    break\n",
        "            reference_list.append(line_sent) # [ [sen1], [sen2], ... ]\n",
        "        hypothesis_list.append(hypothesis)\n",
        "\n",
        "\n",
        "        count += 1\n",
        "        current += len(data)\n",
        "        if batch_idx % log_interval == 0:\n",
        "            total = len(loader.dataset)-np.floor(ratio * len(loader.dataset))\n",
        "            print('[{}/{} ({:.0f}%)]'.format(\n",
        "                current, total,\n",
        "                100. * current / total))\n",
        "\n",
        "    return hypothesis_list, reference_list\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE0sPJWC6cMk"
      },
      "source": [
        "# sentence1 = [['the', 'the', 'the', 'the', 'the', 'cat'], \n",
        "#              ['second', 'sentence', 'does', 'not', 'match']]\n",
        "# sentence2 = [[['the', 'cat', 'is', 'on', 'the', 'mat'], ['the', 'the', 'the', 'the', 'cat']],\n",
        "#              [['second', 'sentence', 'is', 'the', 'best']]]\n",
        "# bleu_score(sentence1, sentence2, max_n=1, weights=[1])\n",
        "\n",
        "# sent1 = [['a', 'dog', 'is', 'happily', 'running', 'on', 'the', 'sand']]\n",
        "# sent2 = [[['a', 'dog', 'is', 'happily','standing', 'on', 'the', 'beach']]]\n",
        "# print(bleu_score(sent1, sent2, max_n=4, weights=[0.25,0.25,0.25,0.25]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCsrTCSFMdPt"
      },
      "source": [
        "### 3.2 Get Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4w9II4SICaY"
      },
      "source": [
        "#### flickr30k ####\n",
        "# Prepare Dataset\n",
        "root_folder = 'flickr30k/images'\n",
        "annotation_file = 'flickr30k/captions.txt'\n",
        "train_dev_ratio = 0.99\n",
        "transform = get_transform()\n",
        "batch_size = 64\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader, dev_loader, dataset = get_loader(root_folder, annotation_file, transform, ratio=train_dev_ratio, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-2CPpyoC6l6"
      },
      "source": [
        "#### flickr8k ####\n",
        "root_folder = 'flickr8k/images'\n",
        "annotation_file = 'flickr8k/captions.txt'\n",
        "train_dev_ratio = 0.95\n",
        "transform = get_transform()\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader, dev_loader, dataset = get_loader(root_folder, annotation_file, transform,\n",
        "                                                          ratio=train_dev_ratio, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH7yRlNYoR2U"
      },
      "source": [
        "# Prepare Glove Embedding\n",
        "glove_embedding = get_glove_embedding()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzzGo2DwMl8t"
      },
      "source": [
        "### 3.3 Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FGIaskEg6Yj"
      },
      "source": [
        "######################################\n",
        "# !!! Section for model creation !!! #\n",
        "######################################\n",
        "\n",
        "# for flickr8k\n",
        "hidden_size = 512 \n",
        "units = 512\n",
        "\n",
        "####### for cp_30k_1 #########\n",
        "# hidden_size = 1024\n",
        "# units = 1024\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(dataset.vocab.itos, glove_embedding) # arguments(list, dictionary)\n",
        "model = CaptionModel(embedding_matrix, hidden_size, units,\n",
        "                     freeze_resnet=True, freeze_embedding=False)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTcVBVz9i_P7"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMNdICD9MvBo"
      },
      "source": [
        "### 3.4 Hyperparameters and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbURHg-oR2V"
      },
      "source": [
        "# model\n",
        "learning_rate = 3e-5\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRHQI0S5auz"
      },
      "source": [
        "train(model, device, train_loader, optimizer, criterion, epoch=5, log_interval=1, save=True, ratio=train_dev_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUu_lgL5SESb"
      },
      "source": [
        "evaluate(model, device, dev_loader, dataset.vocab, ratio=train_dev_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dwwn9QcJS8k"
      },
      "source": [
        "### NLTK load ###\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "hypo, ref = evaluate2(model, device, dev_loader, dataset.vocab, ratio=train_dev_ratio)\n",
        "bleus = [0,0,0,0]\n",
        "for i in range(len(hypo)):\n",
        "    #print(ref[5*i:5*i+5])\n",
        "    #print(hypo[i])\n",
        "    for j in range(1,5):\n",
        "        bleus[j-1] += sentence_bleu(ref[5*i:5*i+5], hypo[i][0], weights=j*[1/j], smoothing_function=SmoothingFunction().method4)\n",
        "        #sentence_bleu\n",
        "print(np.array(bleus)/len(hypo))\n",
        "\n",
        "# array([0.42902065, 0.29507196, 0.24539325, 0.22117131])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0t8cLBwBnNW"
      },
      "source": [
        "# cp_30k_1.pt # 30k\n",
        "# Bleu-1 score: 0.63\n",
        "# Bleu-2 score: 0.422\n",
        "# Bleu-3 score: 0.257\n",
        "# Bleu-4 score: 0.155\n",
        "\n",
        "# cp_h512_u512_resF_embNF_1002.pt\n",
        "# Bleu-1 score: 0.582\n",
        "# Bleu-2 score: 0.372\n",
        "# Bleu-3 score: 0.197\n",
        "# Bleu-4 score: 0.099\n",
        "\n",
        "# cp_h512_u512_resF_embNF_1021.pt\n",
        "# Bleu-1 score: 0.579\n",
        "# Bleu-2 score: 0.36\n",
        "# Bleu-3 score: 0.182\n",
        "# Bleu-4 score: 0.082\n",
        "\n",
        "# cp_h512_u512_resF_embNF_1022_noAtt.pt\n",
        "# Bleu-1 score: 0.461\n",
        "# Bleu-2 score: 0.25\n",
        "# Bleu-3 score: 0.09\n",
        "# Bleu-4 score: 0.04\n",
        "\n",
        "# cp_h512_u512_resF_embN_1.pt\n",
        "# Bleu-1 score: 0.655\n",
        "# Bleu-2 score: 0.475\n",
        "# Bleu-3 score: 0.3\n",
        "# Bleu-4 score: 0.182\n",
        "\n",
        "# cp_h512_u512_resF_embN_trainDev.pt\n",
        "# Bleu-1 score: 0.545\n",
        "# Bleu-2 score: 0.348\n",
        "# Bleu-3 score: 0.181\n",
        "# Bleu-4 score: 0.092"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0wBnhuz5Zcp"
      },
      "source": [
        "## 4. Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFBIa1iF25F7"
      },
      "source": [
        "\n",
        "# plot result\n",
        "def plot_attention(image, result, attention):\n",
        "    temp_image = np.array(image)\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention[i], (7, 7))\n",
        "        grid_size = max(np.ceil(len_result/2), 1)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQOb5DoqoR2Y"
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "transform = get_transform()\n",
        "\n",
        "# imgs: dog, surf, men, random, cutie\n",
        "img1 = Image.open(\"test/surf.jpg\").convert(\"RGB\")\n",
        "img1_tensor = transform(img1) # input for model\n",
        "img1_show = transforms.CenterCrop((224,224))(transforms.Resize(224)(img1)) # for display\n",
        "generated_caption, attention = model.generate_caption(img1_tensor.unsqueeze(0).to(device), dataset.vocab)\n",
        "generated_caption = generated_caption[0]\n",
        "attention = attention[0]\n",
        "plot_attention(image=img1_show, result=generated_caption, attention=attention)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J54oZKxmNL_u"
      },
      "source": [
        "## 5. Save and Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ovbLWrI67Z"
      },
      "source": [
        "#### for cpu ####\n",
        "#torch.save(model.state_dict(), 'cp_h512_u512_resF_embNF_1002.pt')\n",
        "#torch.save(model.state_dict(), 'cp_h512_u512_resF_embNF_1022_noAtt.pt')\n",
        "\n",
        "#model.load_state_dict(torch.load('cp_h512_u512_resF_embNF_1002.pt', map_location=torch.device('cpu')))\n",
        "#model.load_state_dict(torch.load('cp_30k_1.pt', map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQfJxRINJYvJ"
      },
      "source": [
        "#### for gpu ####\n",
        "# model.load_state_dict(torch.load('cp_h512_u512_resF_embN_1.pt'))\n",
        "model.load_state_dict(torch.load('cp_h512_u512_resF_embN_1.pt'))\n",
        "#model.load_state_dict(torch.load('cp_30k_1.pt'))\n",
        "\n",
        "# for cpu\n",
        "#model.load_state_dict(torch.load('cp_h512_u512_resF_embN_1.pt', map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZavUMOOnxn9X"
      },
      "source": [
        "#!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y_7uPVbU02d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}